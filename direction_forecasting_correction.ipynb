{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Personal Use Notice\n",
    "The content of this notebook is intended for private use only. It may be shared with friends and family for non-commercial purposes, but must not be posted online, resold, or distributed publicly in any form. Unauthorized public sharing, uploading, or commercial use is strictly prohibited.\n",
    "\n",
    "Â© 2025 - Jeremy COCHOY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " #!wget https://github.com/jeremycochoy/mlprague-2025-workshop/raw/refs/heads/master/klines_5m_BTC_USDT.parquet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:07.797844Z",
     "end_time": "2025-04-28T14:52:07.911313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "-rulr6-1DFwr",
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:07.800120Z",
     "end_time": "2025-04-28T14:52:07.997536Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_parquet('klines_5m_BTC_USDT.parquet')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:07.800744Z",
     "end_time": "2025-04-28T14:52:08.190681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resample the dataframe to a 30 min frequency. Handle carefully first, last, max and min."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.resample('30min').apply({\n",
    "    'open': 'first',\n",
    "    'close': 'last',\n",
    "    'low': 'min',\n",
    "    'high': 'max',\n",
    "})\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:08.088494Z",
     "end_time": "2025-04-28T14:52:08.301454Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look at the change between open and close of a candle, normalized by the average price of btc at this time. You can take the time to zoom in or out."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = 2 * (df['close'] - df['open']) / (df['close'] + df['open'])\n",
    "sns.histplot(data, kde=True).set(xlim=(-0.04, 0.04))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:08.884848Z",
     "end_time": "2025-04-28T14:52:10.966506Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the direction of the price between two consecutive close. A 1 for a price increasing, a -1 for a price decreasing. If the price remain unchanged, the value is 0. This is the information we aim at forecasting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "delta_price = df['close'] - df['close'].shift(1)\n",
    "direction = (delta_price > 0) * 1.0 - (delta_price < 0) * 1.0\n",
    "direction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:09.716111Z",
     "end_time": "2025-04-28T14:52:10.990591Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use a scatter plot to visualize a small temporal slice and get an idea of the amount of up/down/zero."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(direction.iloc[:250].index, direction.iloc[:250])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:11.515835Z",
     "end_time": "2025-04-28T14:52:12.330086Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualise the occurrence of each class in the whole dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count occurrences\n",
    "values, counts = np.unique(direction, return_counts=True)\n",
    "\n",
    "# Plot\n",
    "plt.bar([str(v) for v in values], counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution (-1, 0, 1)')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:13.650505Z",
     "end_time": "2025-04-28T14:52:13.775928Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute a normalized price change. This is a multiplicative factor between the close price of two consecutive candles. Make sure the new time serie remain causal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "price_change = df['close'] / df['close'].shift(1)\n",
    "price_change = price_change.fillna(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:14.601098Z",
     "end_time": "2025-04-28T14:52:14.612648Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(price_change.iloc[-300:])\n",
    "plt.title('Price change')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:15.023770Z",
     "end_time": "2025-04-28T14:52:15.094183Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:16.567247Z",
     "end_time": "2025-04-28T14:52:18.684297Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a training and validation dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEQ_LEN = 512\n",
    "split_date = '2024-01-01'\n",
    "\n",
    "# Ensure alignment\n",
    "assert price_change.index.equals(direction.index)\n",
    "\n",
    "# Split by date\n",
    "train_mask = price_change.index < split_date\n",
    "val_mask = price_change.index >= split_date\n",
    "\n",
    "price_train, price_val = price_change[train_mask], price_change[val_mask]\n",
    "dir_train, dir_val = direction[train_mask], direction[val_mask]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:19.068471Z",
     "end_time": "2025-04-28T14:52:19.081417Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PriceToDirectionDataset(Dataset):\n",
    "    def __init__(self, price_change_series, direction_series, seq_len=512):\n",
    "        self.price_change = price_change_series.values.astype(np.float32)\n",
    "        self.directions = direction_series.values.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        assert len(self.price_change) == len(self.directions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.price_change) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.price_change[idx:idx + self.seq_len]\n",
    "        y = self.directions[idx + self.seq_len]\n",
    "        return torch.tensor(x).unsqueeze(-1), torch.tensor([y])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:19.703295Z",
     "end_time": "2025-04-28T14:52:19.712043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = PriceToDirectionDataset(price_train, dir_train, seq_len=SEQ_LEN)\n",
    "val_dataset = PriceToDirectionDataset(price_val, dir_val, seq_len=SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:20.468553Z",
     "end_time": "2025-04-28T14:52:20.538164Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "We build a simple model taking the price change as input, and returning two classification scores (logits) representing unnormalized probabilities for the next candle to go up or down."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Forecast(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based sequence model for binary classification of time series direction.\n",
    "\n",
    "    Given a sequence of prices (or features), the model outputs two scores (logits)\n",
    "    at each time step, representing unnormalized probabilities for 'up' and 'down' directions.\n",
    "\n",
    "    Final softmax is applied externally (e.g., in the loss function like nn.CrossEntropyLoss).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=1, hidden_dim=16, num_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features per time step.\n",
    "            hidden_dim (int): Size of the GRU hidden state.\n",
    "            num_layers (int): Number of stacked GRU layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)  # Output logits for two classes: [up, down]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch, sequence_length, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch, sequence_length, 2)\n",
    "                    representing scores for each class at each time step.\n",
    "        \"\"\"\n",
    "        out, _ = self.gru(x)     # out shape: (batch, seq_len, hidden_dim)\n",
    "        out = self.fc(out)       # out shape: (batch, seq_len, 2)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:22.502318Z",
     "end_time": "2025-04-28T14:52:22.519472Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate some noise and run it through the model to confirm the proper implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.randn(2, 512, 1)  # batch of 2, seq len 512\n",
    "model = Forecast()\n",
    "y = model(x)\n",
    "print(y)  # should be shaped (2, 512, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:23.858356Z",
     "end_time": "2025-04-28T14:52:23.894476Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Training setup for this model. We use a cross entropy loss to predict the direction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, lr=1e-3, loss_fn=None):\n",
    "        \"\"\"\n",
    "        Trainer for binary classification (up/down) based on GRU outputs.\n",
    "\n",
    "        Model output: logits of shape (batch, time, 2)\n",
    "        Target: float labels â mapped to class index:\n",
    "            y >= 0 â 0 (up)\n",
    "            y <  0 â 1 (down)\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = loss_fn if loss_fn else nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_loss_steps = []\n",
    "        self.val_loss_checkpoints = []\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "            val_check_interval = max(1, len(self.train_loader) // 10)\n",
    "\n",
    "            for i, (x, y) in enumerate(pbar):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                # Convert float targets to class indices: y >= 0 â 0, y < 0 â 1\n",
    "                y_class = (y.squeeze() < 0).long()  # True (down) â 1, False (up) â 0\n",
    "\n",
    "                logits = self.model(x)[:, -1, :]  # shape: (batch, 2)\n",
    "                loss = self.loss_fn(logits, y_class)\n",
    "\n",
    "                assert not torch.isnan(x).any().any()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "\n",
    "                self.train_loss_steps.append(loss.item())\n",
    "\n",
    "                if (i + 1) % val_check_interval == 0:\n",
    "                    val_loss = self.evaluate()\n",
    "                    self.val_loss_checkpoints.append(val_loss)\n",
    "                else:\n",
    "                    val_loss = self.val_loss_checkpoints[-1] if len(self.val_loss_checkpoints) > 0 else np.nan\n",
    "                pbar.set_postfix(train_loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_class = (y.squeeze() < 0).long()\n",
    "                logits = self.model(x)[:, -1, :]\n",
    "                loss = self.loss_fn(logits, y_class)\n",
    "                val_losses.append(loss.item())\n",
    "        self.model.train()\n",
    "\n",
    "        return sum(val_losses) / len(val_losses)\n",
    "\n",
    "    def plot_losses(self, smooth_window=1024):\n",
    "        \"\"\"\n",
    "        Plots training loss (smoothed) and validation loss.\n",
    "        Aligns both curves over the same training duration.\n",
    "\n",
    "        Args:\n",
    "            smooth_window (int): Window size for moving average on train loss.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Smooth train loss using moving average\n",
    "        if len(self.train_loss_steps) >= smooth_window:\n",
    "            kernel = np.ones(smooth_window) / smooth_window\n",
    "            smoothed_train = np.convolve(self.train_loss_steps, kernel, mode='valid')\n",
    "            smoothed_x = np.arange(len(smoothed_train)) + smooth_window // 2\n",
    "            plt.plot(smoothed_x, smoothed_train, label=f\"Train Loss (smoothed, {smooth_window})\")\n",
    "        else:\n",
    "            smoothed_x = np.arange(len(self.train_loss_steps))\n",
    "            plt.plot(smoothed_x, self.train_loss_steps, label=\"Train Loss\")\n",
    "\n",
    "        # Rescale val x to match train loss duration\n",
    "        val_x = np.linspace(smoothed_x[0], smoothed_x[-1], len(self.val_loss_checkpoints))\n",
    "        plt.plot(val_x, self.val_loss_checkpoints, 'o-', label=\"Val Loss\")\n",
    "\n",
    "        plt.title(\"Training & Validation Loss\")\n",
    "        plt.xlabel(\"Training Steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:26.267135Z",
     "end_time": "2025-04-28T14:52:26.325943Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_loader, val_loader, lr=1e-3)\n",
    "trainer.train(epochs=7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-10T15:28:44.700583Z",
     "end_time": "2025-04-10T15:45:21.079820Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.plot_losses()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:44.632694Z",
     "end_time": "2025-04-28T14:52:44.706427Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a one layer model. Compare with a 3 layer. Try to double or divide by two the latent dimension. Observe how both the train curve and validation change."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 2\n",
    "\n",
    "## Backtest\n",
    "\n",
    "We would like to estimate what would be our profits if we try to trade according to the direction predicted every 30 min. Remember that if we trade futures, we can short the asset.\n",
    "\n",
    "There is two simple policies we can design to trade this information:\n",
    "1. Each 30 minutes, we allocate all our remaining capital to a directional move aligned with the highest score\n",
    "2. Each 30 minutes, we compute the probability for the price increasing $p_{up}$ and the price decreasing $p_{down}$ and substract them to obtain a directional multiplicative factor of the capital we want to allocate $-1 \\leq p_{up}-p_{down} \\leq 1$.\n",
    "\n",
    "We assume:\n",
    "* No transaction fee, spread or slipage\n",
    "* Trades are always executed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def backtest_log_return(model, prices: pd.Series) -> float:\n",
    "    log_prices = np.log(prices.values)\n",
    "\n",
    "    model_input = torch.Tensor(prices.values[:-1]).to(device).reshape(-1, 1)\n",
    "    logits = model(model_input)  # shape: (T-1, 2)\n",
    "    probs = torch.softmax(logits, axis=1).detach().cpu().numpy()\n",
    "    trades = (probs[:, 0] >= 0.5) * 1.0 - (probs[:, 1] >= 0.5)  # long - short\n",
    "    log_returns = log_prices[1:] * trades\n",
    "    return log_returns.cumsum(), log_prices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:48.328761Z",
     "end_time": "2025-04-28T14:52:48.407091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_disc, log_prices = backtest_log_return(model, price_val)\n",
    "r_disc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:49.162423Z",
     "end_time": "2025-04-28T14:52:50.276862Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(r_disc, label='discrete')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-28T14:52:52.681272Z",
     "end_time": "2025-04-28T14:52:52.863790Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of training the model with the cross entropy loss, formulate a loss based on the profits and train directly an \"end-to-end\" model. How does the profit and risk profile on the train set change? What is the impact on the validation set?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
